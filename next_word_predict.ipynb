{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pickle\n",
    "import codecs\n",
    "import lxml.etree as ET\n",
    "import regex\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = regex.sub(\"\\[http[^]]+? ([^]]+)]\", r\"\\1\", text) \n",
    "    text = regex.sub(\"\\[http[^]]+]\", \"\", text) \n",
    "    text = regex.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n",
    "    text = regex.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n",
    "    text = regex.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n",
    "    text = regex.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n",
    "    text = regex.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n",
    "    \n",
    "    text = regex.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n",
    "    text = regex.sub(\"[']{3}\", \"\", text) # remove bold symbols\n",
    "    text = regex.sub(\"[']{2}\", \"\", text) # remove italic symbols\n",
    "    \n",
    "    text = regex.sub(u\"[^ \\r\\n\\p{Latin}\\d\\-'.?!]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = regex.sub(\"[ ]{2,}\", \" \", text) # Squeeze spaces.\n",
    "    return text\n",
    "\n",
    "def build_corpus():\n",
    "    import glob\n",
    "    \n",
    "    with codecs.open('data/en_wikinews.txt', 'w', 'utf-8') as fout:\n",
    "        fs = glob.glob('data/raw/*.xml')\n",
    "        ns = \"{http://www.mediawiki.org/xml/export-0.10/}\" # namespace\n",
    "        for f in fs:\n",
    "            i = 1\n",
    "            for _, elem in ET.iterparse(f, tag=ns+\"text\"):\n",
    "                try:\n",
    "                    if i > 5000:\n",
    "                        running_text = elem.text\n",
    "                        running_text = running_text.split(\"===\")[0]\n",
    "                        running_text = clean_text(running_text)\n",
    "                        paras = running_text.split(\"\\n\")\n",
    "                        for para in paras:\n",
    "                            if len(para) > 500:\n",
    "                                sents = [regex.sub(\"([.!?]+$)\", r\" \\1\", sent) for sent in sent_tokenize(para.strip())]\n",
    "                                fout.write(\" \".join(sents) + \"\\n\")\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                elem.clear() # We need to save memory!\n",
    "                i += 1\n",
    "                if i % 1000 == 0: print(i,)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    build_corpus()\n",
    "    print(\"Done\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (0,) \n",
      "Y.shape = (0,)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "class Hyperparams:\n",
    "    '''Hyper parameters'''\n",
    "    batch_size = 64\n",
    "    embed_dim = 300\n",
    "    seqlen = 50  # We will predict the next/current word based on the preceding 50 characters.\n",
    "\n",
    "def load_char_vocab():\n",
    "    vocab = \"EU abcdefghijklmnopqrstuvwxyz0123456789-.,?!'\" # E: Empty, U:Unknown\n",
    "    char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx:char for idx, char in enumerate(vocab)}  \n",
    "    \n",
    "    return char2idx, idx2char      \n",
    "\n",
    "def create_word_vocab():\n",
    "    from collections import Counter\n",
    "    from itertools import chain\n",
    "    \n",
    "    words = codecs.open('data/en_wikinews.txt', 'r', 'utf-8').read().split()\n",
    "    word2cnt = Counter(chain(words))\n",
    "    vocab = [\"<EMP>\", \"<UNK>\"] + [word for word, cnt in word2cnt.items() if cnt > 50]\n",
    "    word2idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx:word for idx, word in enumerate(vocab)} \n",
    "    pickle.dump( (word2idx, idx2word), open(\"data/word_vocab.pkl\", \"wb\") )\n",
    "\n",
    "def load_word_vocab():\n",
    "    word2idx, idx2word = pickle.load( open(\"data/word_vocab.pkl\", \"rb\") )\n",
    "    return word2idx, idx2word\n",
    "    \n",
    "def create_data():\n",
    "    char2idx, idx2char = load_char_vocab()\n",
    "    word2idx, idx2word = load_word_vocab()\n",
    "    lines = codecs.open('data/en_wikinews.txt', 'r', 'utf-8').read().splitlines()\n",
    "    xs, ys = [], [] # vectorized sentences\n",
    "    for line in lines:\n",
    "        x, y = [], []\n",
    "        for i, word in enumerate(line.split()):\n",
    "            x.append(2) # space\n",
    "            y.append(word2idx.get(word, 1))\n",
    "            for char in word:\n",
    "                x.append(char2idx.get(char, 1))\n",
    "                y.append(word2idx.get(word, 1))\n",
    "        if len(x) <= 1000: #zero pre-padding\n",
    "            xs.append([0] * (1000 - len(x)) + x)\n",
    "            ys.append([0] * (1000 - len(x)) + y)\n",
    "  \n",
    "    # Convert to 2d-arrays\n",
    "    X = np.array(xs)\n",
    "    Y = np.array(ys)\n",
    "    \n",
    "    print(\"X.shape =\", X.shape, \"\\nY.shape =\", Y.shape)\n",
    "    np.savez('data/train.npz', X=X, Y=Y)\n",
    "\n",
    "def load_train_data():\n",
    "    X = np.load('data/train.npz')['X'][:-64]\n",
    "    Y = np.load('data/train.npz')['Y'][:-64]\n",
    "    return X, Y\n",
    "\n",
    "def load_test_data():\n",
    "    X = np.load('data/train.npz')['X'][-64:]\n",
    "    Y = np.load('data/train.npz')['Y'][-64:]\n",
    "    return X, Y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_word_vocab()\n",
    "    create_data()\n",
    "    print(\"Done\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from prepro import *\n",
    "import sugartensor as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_process(t1, t2):\n",
    "    '''\n",
    "    Processes each training sample so that it fits in the queue.\n",
    "    '''\n",
    "    # Lstrip zeros\n",
    "    zeros = tf.equal(t1, tf.zeros_like(t1)).sg_int().sg_sum()\n",
    "    t1 = t1[zeros:] \n",
    "    t2 = t2[zeros:]\n",
    "\n",
    "    # zero-PrePadding\n",
    "    t1 = tf.concat([tf.zeros([Hyperparams.seqlen-1], tf.int32), t1], 0)# 49 zero-prepadding\n",
    "    t2 = tf.concat([tf.zeros([Hyperparams.seqlen-1], tf.int32), t2], 0)# 49 zero-prepadding\n",
    "    # radom crop    \n",
    "    stacked = tf.stack((t1, t2))\n",
    "    cropped = tf.random_crop(stacked, [2, Hyperparams.seqlen])\n",
    "    t1, t2 = cropped[0], cropped[1]\n",
    "    \n",
    "    t2 = t2[-1]\n",
    "\n",
    "    return t1, t2\n",
    "\n",
    "def get_batch_data():\n",
    "    '''Makes batch queues from the data.\n",
    "    '''\n",
    "    # Load data\n",
    "    X, Y = load_train_data() # (196947, 1000) int64\n",
    "\n",
    "    # Create Queues\n",
    "    x_q, y_q = tf.train.slice_input_producer([tf.convert_to_tensor(X, tf.int32),\n",
    "                                          tf.convert_to_tensor(Y, tf.int32)]) # (1000,) int32\n",
    "    \n",
    "    x_q, y_q = q_process(x_q, y_q) # (50,) int32, () int32\n",
    "\n",
    "    # create batch queues\n",
    "    x, y = tf.train.shuffle_batch([x_q, y_q],\n",
    "                              num_threads=32,\n",
    "                              batch_size=Hyperparams.batch_size, \n",
    "                              capacity=Hyperparams.batch_size*64,\n",
    "                              min_after_dequeue=Hyperparams.batch_size*32, \n",
    "                              allow_smaller_final_batch=False)\n",
    "    \n",
    "    num_batch = len(X) // Hyperparams.batch_size\n",
    "\n",
    "    return x, y, num_batch # (64, 50) int32, (64, 50) int32, ()\n",
    "\n",
    "class ModelGraph():\n",
    "    '''Builds a model graph'''\n",
    "    def __init__(self, mode=\"train\"):\n",
    "        '''\n",
    "        Args:\n",
    "          mode: A string. Either \"train\" or \"test\"\n",
    "        '''\n",
    "        self.char2idx, self.idx2char = load_char_vocab()\n",
    "        self.word2idx, self.idx2word = load_word_vocab()\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            self.x, self.y, self.num_batch = get_batch_data() \n",
    "        else:\n",
    "            self.x = tf.placeholder(tf.int32, [None, Hyperparams.seqlen])\n",
    "        \n",
    "        self.emb_x = tf.sg_emb(name='emb_x', voca_size=len(self.char2idx), dim=Hyperparams.embed_dim)\n",
    "        self.enc = self.x.sg_lookup(emb=self.emb_x)\n",
    "        \n",
    "        with tf.sg_context(size=5, act='relu', bn=True):\n",
    "            for _ in range(20):\n",
    "                dim = self.enc.get_shape().as_list()[-1]\n",
    "                self.enc += self.enc.sg_conv1d(dim=dim) # (64, 50, 300) float32\n",
    "        \n",
    "        self.enc = self.enc.sg_conv1d(size=1, dim=len(self.word2idx), act='linear', bn=False) # (64, 50, 21293) float32\n",
    "#         self.logits = self.enc.sg_mean(dims=[1], keep_dims=False) # (64, 21293) float32\n",
    "        \n",
    "        # Weighted Sum. Updated on Feb. 15, 2017.\n",
    "        def make_weights(size):\n",
    "            weights = tf.range(1, size+1, dtype=tf.float32)\n",
    "            weights *= 1. / ((1 + size) * size // 2)\n",
    "            weights = tf.expand_dims(weights, 0)\n",
    "            weights = tf.expand_dims(weights, -1)\n",
    "            return weights\n",
    "        \n",
    "        self.weights = make_weights(Hyperparams.seqlen) # (1, 50, 1)\n",
    "        self.enc *= self.weights # Broadcasting\n",
    "        self.logits = self.enc.sg_sum(axis=[1], keep_dims=False) # (64, 21293)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.ce = self.logits.sg_ce(target=self.y, mask=False, one_hot=False)\n",
    "            self.istarget = tf.not_equal(self.y, tf.ones_like(self.y)).sg_float() # 1: Unkown   \n",
    "            self.reduced_loss = ((self.ce * self.istarget).sg_sum()) / (self.istarget.sg_sum() + 1e-5)\n",
    "            tf.sg_summary_loss(self.reduced_loss, \"reduced_loss\")\n",
    "            \n",
    "def train():\n",
    "    g = ModelGraph()\n",
    "    print(\"Graph loaded!\")\n",
    "\n",
    "    tf.sg_train(optim=\"Adam\", lr=0.00001, lr_reset=True, loss=g.reduced_loss, eval_metric=[], max_ep=20000, \n",
    "                save_dir='asset/train', early_stop=False, ep_size=g.num_batch)\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    train(); print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
